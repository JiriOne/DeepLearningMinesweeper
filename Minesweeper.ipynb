{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from NN import Agent\n", "import copy\n", "import matplotlib.pyplot as plt "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_board(w, h, bombs):\n", "    board = np.zeros((w,h))\n", "    \n", "    # Place bombs\n", "    bombs = np.random.choice(w*h, bombs, replace=False)\n", "    for bomb in bombs:\n", "        x = bomb // w\n", "        y = bomb % w\n", "        board[x][y] = -1\n\n", "    # Place numbers\n", "    for i in range(w):\n", "        for j in range(h):\n", "            if board[i][j] == -1:\n", "                continue\n", "            count = 0\n", "            for x in range(max(0, i-1), min(w, i+2)):\n", "                for y in range(max(0, j-1), min(h, j+2)):\n", "                    if board[x][y] == -1:\n", "                        count += 1\n", "            board[i][j] = count\n", "    return board"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def action(board, visible_board, x, y):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    if board[x][y] == -1:\n", "        return False\n", "    visible_board[x][y] = board[x][y]\n", "    if board[x][y] == 0:\n", "        for i in range(max(0, x-1), min(len(board), x+2)):\n", "            for j in range(max(0, y-1), min(len(board[0]), y+2)):\n", "                if visible_board[i][j] == -2:\n", "                    action(board, visible_board, i, j)\n", "    \n", "    return True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def print_board(board):\n", "    for row in board:\n", "        for cell in row:\n", "            if cell == -2:\n", "                print(\"?\", end=\" \")\n", "            elif cell == -1:\n", "                print(\"X\", end=\" \")\n", "            else:\n", "                print(int(cell), end=\" \")\n", "        print()\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n\n", "    #initialize the board\n", "    w,h = 5,5\n", "    reward_list = []\n", "    trailing_reward = []\n", "    n_action_list = []\n", "    win_ratio_list = []\n", "    trailing_wl = []\n\n", "    #winratio last 100 games\n", "    win_loss_ratio = np.zeros(100)\n", "    n_games = 1000\n", "    wins = 0\n\n", "    #initialize the agent\n", "    agent = Agent(gamma=0.99, epsilon=1.0,batch_size=64, n_actions=w*h,eps_end=0.01,input_dims=[w*h],lr=0.001,eps_dec=(1/n_games),max_mem_size=100000)\n\n", "    #load the model\n", "    # agent = Agent(gamma=0.99, epsilon=0.0,batch_size=64, n_actions=w*h,eps_end=0.01,input_dims=[w*h],lr=0.04,eps_dec=(1/n_games),max_mem_size=50000)\n", "    # agent = agent.load_model(name=\"model_300000.pt\")\n", "    for i in range(n_games):\n", "        done = False\n", "        board = create_board(w,h,bombs=3)\n", "        visible_board = np.zeros((w,h))\n", "        visible_board.fill(-2)\n", "        n_actions = 0\n", "        actions_taken = []\n", "        while (not done and n_actions < 25):            \n", "            #print_board(visible_board)\n", "            #human play\n", "            #x = int(input(\"Enter x: \"))\n", "            #y = int(input(\"Enter y: \"))\n\n", "            #next = input(\"go next?\")\n\n", "            #Choose the action with the agent\n", "            curr_action = agent.choose_action(visible_board.flatten())\n\n", "            #store the current state before the action\n", "            curr_state = copy.deepcopy(visible_board)\n\n", "            #initialize rewards\n", "            curr_reward = 0\n", "            reward_terminal = 0\n", "            reward_cells = 0\n", "            reward_new_action = 0\n\n", "            #store the action taken\n", "            actions_taken.append(curr_action)\n", "         \n", "            #get the x and y coordinates from the action\n", "            x = curr_action // w\n", "            y = curr_action % w\n\n", "            #print(\"x: \", x, \"y: \", y)\n\n", "            #loss\n", "            if not action(board, visible_board, x, y):\n", "                done = True   \n", "                reward_terminal = -1\n", "                win_loss_ratio[i % 100] = 0\n\n", "            #win\n", "            if np.count_nonzero(visible_board == -2) == 3:\n", "                done = True\n", "                wins += 1\n", "                reward_terminal = 1\n", "                win_loss_ratio[i % 100] = 1   \n", "            \n", "            if not done:\n", "                cells_opened = 0\n", "                #reward for each new tile opened\n", "                for a in range(w):\n", "                    for b in range(h):\n", "                        if visible_board[a][b] != -2 and curr_state[a][b] == -2:\n", "                            cells_opened += 1\n", "            \n", "                reward_cells = cells_opened / w*h\n", "                if curr_action in actions_taken:\n", "                    reward_new_action = -1\n\n", "            #reward function\n", "            curr_reward = 100*reward_terminal + 10*reward_cells + 25*reward_new_action\n", "            n_actions += 1\n", "            \n", "            #store the transition\n", "            agent.store_transition(curr_state.flatten(), curr_action, curr_reward, visible_board.flatten(), done)  \n\n", "            #update the reward lists\n", "            reward_list.append(curr_reward)\n", "            trailing_reward.append(np.mean(reward_list[-1000:]))\n", "                  \n", "        #update the number of actions list\n", "        n_action_list.append(n_actions)       \n\n", "        #agent.learn()\n", "        win_loss_ratio_number = np.sum(win_loss_ratio) / 100\n", "        win_ratio_list.append(win_loss_ratio_number)\n", "        trailing_wl.append(np.mean(win_ratio_list[-1000:]))\n", "        if i % 100 == 0:\n", "            \n", "            print(f\"game: {i} trailing_reward: {np.mean(reward_list[-100:]):.2f} epsilon: {agent.epsilon:.2f} actions: {n_actions} wins: {wins} win_loss_ratio: {win_loss_ratio_number:.2f}\")\n\n", "        # if i % 100_000 == 0:\n", "        #     #save model with i name\n", "        #     agent.save_model(name=f\"model_{i}.pt\")\n", "    #plot trailing reward\n", "    plt.plot(trailing_wl)\n", "    plt.savefig(\"winpercentage.png\")\n", "    \n", "    #agent.save_model()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}