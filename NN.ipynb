{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["ode based on https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch as T\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepQNetwork(nn.Module):\n", "    #initializing the network\n", "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, fc3_dims, n_actions):\n", "        super(DeepQNetwork, self).__init__()\n", "        self.input_dims = input_dims\n", "        self.fc1_dims = fc1_dims\n", "        self.fc2_dims = fc2_dims\n", "        self.fc3_dims = fc3_dims\n", "        self.n_actions = n_actions\n", "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n", "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n", "        self.fc3 = nn.Linear(self.fc2_dims, self.fc3_dims)\n", "        self.fc4 = nn.Linear(self.fc3_dims, self.n_actions)\n", "        self.optimizers = optim.Adam(self.parameters(), lr=lr)\n", "        self.loss = nn.MSELoss()\n", "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')  #\n", "        self.to(self.device)\n\n", "    #forward propagation\n", "    def forward(self, state):\n", "        x = F.relu(self.fc1(state))\n", "        x = F.relu(self.fc2(x))\n", "        x = F.relu(self.fc3(x))\n", "        actions = self.fc4(x)\n", "        return actions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Agent():\n", "    #initializing the agent\n", "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,max_mem_size=10000,eps_end=0.01, eps_dec=2e-5):\n", "        self.gamma = gamma\n", "        self.epsilon = epsilon\n", "        self.eps_min = eps_end\n", "        self.eps_dec = eps_dec\n", "        self.lr = lr\n", "        self.action_space = [i for i in range(n_actions)] \n", "        self.mem_size = max_mem_size\n", "        self.batch_size = batch_size\n", "        self.mem_cntr = 0\n", "        self.Q_eval = DeepQNetwork(self.lr, n_actions=n_actions, input_dims=input_dims, fc1_dims=64, fc2_dims=64, fc3_dims=64)\n", "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n", "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n", "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n", "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n", "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n\n", "    #storing the transition in the memory\n", "    def store_transition(self, state, action, reward, state_, done):\n", "        index = self.mem_cntr % self.mem_size\n", "        self.state_memory[index] = state\n", "        self.new_state_memory[index] = state_\n", "        self.reward_memory[index] = reward\n", "        self.action_memory[index] = action\n", "        self.terminal_memory[index] = done\n", "        \n", "        self.mem_cntr += 1\n\n", "    #choosing the action\n", "    def choose_action(self, observed_state):\n", "        if np.random.random() > self.epsilon:\n", "            \n", "            state = T.tensor(observed_state, dtype=T.float).to(self.Q_eval.device)\n", "            actions = self.Q_eval.forward(state)\n", "            action = T.argmax(actions).item()\n", "        else:\n", "            action = np.random.choice(self.action_space)\n", "        return action\n", "    \n", "    #learning from the memory\n", "    def learn(self):\n", "        if self.mem_cntr < self.batch_size:\n", "            return\n", "        self.Q_eval.optimizers.zero_grad()\n", "        max_mem = min(self.mem_cntr, self.mem_size)\n", "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n", "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n", "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n", "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n", "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n", "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n", "        action_batch = self.action_memory[batch]\n", "        q_eval = self.Q_eval.__call__(state_batch)[batch_index,action_batch]    #forward(state_batch)[batch_index,action_batch]\n", "        q_next = self.Q_eval.__call__(new_state_batch)                          #forward(new_state_batch)\n", "        q_next[terminal_batch] = 0.0\n", "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n", "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n", "        loss.backward()\n", "        self.Q_eval.optimizers.step()\n", "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n\n", "    #save and load the model\n", "    def save_model(self, name='model.pt'):\n", "        T.save(self.Q_eval.state_dict(), name)\n", "    def load_model(self, name='model.pt'):\n", "        self.Q_eval.load_state_dict(T.load(name))\n", "        self.Q_eval.eval()\n", "        return self\n", "        "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}